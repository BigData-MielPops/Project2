{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import postgres_import as pos_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_command(command, verbose=True, save_mem=False):\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "    returncode = process.wait()\n",
    "    output = \"\"\n",
    "    if not(save_mem):\n",
    "        output = process.stdout.read().decode('utf-8', 'replace').replace(\"\\r\", \"\").split(\"\\n\")\n",
    "        if verbose: \n",
    "            for line in output:\n",
    "                print (line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating redis ... \n",
      "Creating postgres ... \n",
      "Creating memsql ... \n",
      "Creating cockroach ... \n",
      "Creating nuodb ... \n",
      "Creating mysql ... \n",
      "Creating cassandra ... \n",
      "Creating neo4j ... \n",
      "Creating voltdb ... \n",
      "Creating mongo ... \n",
      "Creating orientdb ... \n",
      "Creating trafodion ... \n",
      "Creating postgres\n",
      "Creating cockroach\n",
      "Creating redis\n",
      "Creating mongo\n",
      "Creating memsql\n",
      "Creating nuodb\n",
      "Creating cassandra\n",
      "Creating orientdb\n",
      "Creating trafodion\n",
      "Creating mysql\n",
      "Creating neo4j\n",
      "Creating voltdb\n",
      "\u001b[1A\u001b[2KCreating postgres ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating redis ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating cockroach ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating mongo ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating neo4j ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating cassandra ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating trafodion ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating orientdb ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating memsql ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating nuodb ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating mysql ... \u001b[32mdone\u001b[0m\u001b[1B\u001b[1A\u001b[2KCreating voltdb ... \u001b[32mdone\u001b[0m\u001b[1B\n"
     ]
    }
   ],
   "source": [
    "o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose up -d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "containers = [\"mongo\", \"redis\", \"neo4j\", \"orientdb\", \"cassandra\", \"postgres\", \"mysql\", \"memsql\", \"voltdb\", \"cockroach\"]\n",
    "data_import_commands = [\n",
    "    \"mongoimport --db testing_db --collection customers --type csv --headerline --file /shared_data/mock_data_10000_rows.csv\",\n",
    "    \"cat /shared_data/mock_data_10000_rows2.csv | redis-cli --pipe\",\n",
    "    \"/var/lib/neo4j/bin/neo4j-admin import --mode=csv --database=mock.db  --nodes:Customer /shared_data/mock_data_10000_neo.csv --id-type=string\",\n",
    "    \"/orientdb/bin/oetl.sh /shared_scripts/import_orient_10000.json\",\n",
    "    \"cqlsh -f /shared_scripts/import_mock_10000.cql\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\"\n",
    "]\n",
    "import_times = [0] * 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exec_and_time(container, command, verbose=True, save_mem=False):\n",
    "    start = time.time()\n",
    "    if container == \"postgres\":\n",
    "        pos_i.sql_import(os.path.join(os.path.abspath('./Docker/shared-data'),'mock_data_10000_rows_utf8.csv'))\n",
    "    else: \n",
    "        o = run_command('docker exec --privileged ' + container + ' sh -c \"' + command + '\"', verbose, save_mem)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_import_time(iterations):\n",
    "    for j in range(0, iterations):\n",
    "        print(\"iteration \" + str(j))\n",
    "        o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose up -d', verbose=False)\n",
    "        time.sleep(25) \n",
    "        for i in range(0, len(containers)):\n",
    "            import_times[i] += exec_and_time(containers[i], data_import_commands[i], verbose=False, save_mem=True)\n",
    "        o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose down', verbose=False)\n",
    "    for i in range(0, len(containers)):\n",
    "        import_times[i] = import_times[i]/iterations\n",
    "        print(containers[i] + \": \" + str(import_times[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "mongo: 0.3086248397827148\n",
      "redis: 0.1724081039428711\n",
      "neo4j: 2.0556509494781494\n",
      "orientdb: 2.1549182891845704\n",
      "cassandra: 0.6314767837524414\n",
      "postgres: 6.28806848526001\n",
      "mysql: 0.19744563102722168\n",
      "memsql: 0.07149972915649414\n",
      "voltdb: 0.16382579803466796\n",
      "cockroach: 0.14840278625488282\n"
     ]
    }
   ],
   "source": [
    "average_import_time(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing network docker_default\n",
      "Network docker_default not found.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
