{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import postgres_import as pos_i\n",
    "import mysql_import as my_i\n",
    "import memsql_import as mem_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_command(command, verbose=True, save_mem=False):\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, shell=True)\n",
    "    returncode = process.wait()\n",
    "    output = \"\"\n",
    "    if not(save_mem):\n",
    "        output = process.stdout.read().decode('utf-8', 'replace').replace(\"\\r\", \"\").split(\"\\n\")\n",
    "        if verbose: \n",
    "            for line in output:\n",
    "                print (line)\n",
    "    return output\n",
    "\n",
    "def run_command2(command, verbose=True, save_mem=False):\n",
    "    process = subprocess.Popen(command, stdout=subprocess.PIPE, shell=True)\n",
    "    returncode = process.wait()\n",
    "    output = \"\"\n",
    "    if not(save_mem):\n",
    "        output = process.stdout.read().decode('utf-8', 'replace').replace(\"\\r\", \"\").split(\"\\n\")\n",
    "        if verbose: \n",
    "            for line in output:\n",
    "                print (line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neo4j is up-to-date\n",
      "Creating nuodb\n",
      "Creating cockroach\n",
      "Creating redis\n",
      "Creating mysql\n",
      "Creating cassandra\n",
      "Creating trafodion\n",
      "Creating voltdb\n",
      "Creating mongo\n",
      "Creating postgres\n",
      "Creating orientdb\n",
      "Creating memsql\n",
      "\n"
     ]
    }
   ],
   "source": [
    "o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose up -d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "time.sleep(25) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ip = run_command('docker exec cockroach ip route', verbose=False)\n",
    "ip = ip[len(ip)-2].split(\" \")\n",
    "ip = ip[len(ip)-2]\n",
    "coackroach_ip = ip\n",
    "print(coackroach_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "postgres_container = \"postgres\"\n",
    "containers = [\"mongo\", \"redis\", \"neo4j\", \"orientdb\", \"cassandra\", postgres_container, \"mysql\", \"memsql\", \"voltdb\", \"cockroach\"]\n",
    "data_import_commands = [\n",
    "    \"mongoimport --db testing_db --collection customers --type csv --headerline --file /shared_data/mock_data_10000_rows.csv\",\n",
    "    \"cat /shared_data/mock_data_10000_rows2.csv | redis-cli --pipe\",\n",
    "    \"cp /shared_data/mock_data_10000_neo.csv /var/lib/neo4j/import && cat /shared_scripts/import_mock_10000.cypher | /var/lib/neo4j/bin/cypher-shell\",\n",
    "    \"/orientdb/bin/oetl.sh /shared_scripts/import_orient_10000.json\",\n",
    "    [\"bash\", \"cqlsh -f /shared_scripts/import_mock_10000.cql\"],\n",
    "    [pos_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #postgres\n",
    "    [my_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #mysql\n",
    "    [mem_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #memsql\n",
    "    \"cat /shared_scripts/import_voltdb.sql | sqlcmd && /opt/voltdb/bin/csvloader customers -f /shared_data/mock_data_10000_rows.csv\",\n",
    "    [\"./cockroach sql --insecure --execute='create database if not exists mock;'\", \"psql -p 26257 -h '+ coackroach_ip +' -d mock -U root < /shared_data/postgres_dump.sql\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def exec_and_time(container, command, verbose=True, save_mem=False):\n",
    "    start = time.time()\n",
    "    if container == postgres_container or container == \"mysql\" or container == \"memsql\":\n",
    "        command[0](os.path.join(os.path.abspath('./Docker/shared_data'),command[1]))\n",
    "    elif container == \"cockroach\":\n",
    "        o = run_command('docker exec --privileged ' + container + ' sh -c \"' + command[0] + '\"', verbose, save_mem)\n",
    "        o = run_command('docker exec --privileged --user postgres '+ postgres_container +' sh -c \"' + command[1] + '\"', verbose, save_mem)\n",
    "    elif container == \"cassandra\":\n",
    "        o = run_command2('docker exec --privileged ' + container + ' '+ command[0] +' -c \"' + command[1] + '\"', verbose, save_mem)\n",
    "    else: \n",
    "        o = run_command('docker exec --privileged ' + container + ' sh -c \"' + command + '\"', verbose, save_mem)\n",
    "    end = time.time()\n",
    "    return end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def average_time(commands, containers, iterations, verbose=True, import_data=False):\n",
    "    times = [0] * len(containers)\n",
    "    for j in range(0, iterations):\n",
    "        if verbose:\n",
    "            print(\"iteration \" + str(j+1))\n",
    "        o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose up -d', verbose=False)\n",
    "        if verbose:\n",
    "            print(\"waiting...\")\n",
    "        time.sleep(25) \n",
    "        for i in range(0, len(containers)):\n",
    "            if verbose:\n",
    "                print(containers[i])\n",
    "            if import_data:\n",
    "                exec_and_time(containers[i], data_import_commands[i], verbose=False, save_mem=True)\n",
    "            times[i] += exec_and_time(containers[i], commands[i], verbose=False, save_mem=True)\n",
    "        o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose down', verbose=False)\n",
    "    for i in range(0, len(containers)):\n",
    "        times[i] = times[i]/iterations\n",
    "        print(containers[i] + \": \" + str(times[i]))\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import_times = average_time(data_import_commands, containers, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_by_key_commands = [\n",
    "    \"mongoimport --db testing_db --collection customers --type csv --headerline --file /shared_data/mock_data_10000_rows.csv\",\n",
    "    \"cat /shared_data/mock_data_10000_rows2.csv | redis-cli --pipe\",\n",
    "    \"/var/lib/neo4j/bin/neo4j-admin import --mode=csv --database=mock.db  --nodes:Customer /shared_data/mock_data_10000_neo.csv --id-type=string\",\n",
    "    \"/orientdb/bin/oetl.sh /shared_scripts/import_orient_10000.json\",\n",
    "    [\"bash\", \"cqlsh -f /shared_scripts/import_mock_10000.cql\"],\n",
    "    [pos_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #postgres\n",
    "    [my_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #mysql\n",
    "    [mem_i.sql_import, \"mock_data_10000_rows_utf8.csv\"], #memsql\n",
    "    \"cat /shared_scripts/import_voltdb.sql | sqlcmd && /opt/voltdb/bin/csvloader customers -f /shared_data/mock_data_10000_rows.csv\",\n",
    "    [\"./cockroach sql --insecure --execute='create database if not exists mock;'\", \"psql -p 26257 -h '+ coackroach_ip +' -d mock -U root < /shared_data/postgres_dump.sql\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_by_key_times = average_time(get_by_key_commands, containers, 5, import_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = run_command('D: && cd \"D:\\GitHub\\BigData\\Project2\\Docker\" && docker-compose down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
